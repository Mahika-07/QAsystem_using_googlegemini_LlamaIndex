{"docstore/metadata": {"3655a5b6-96d3-4a85-bccc-610d5effef2d": {"doc_hash": "5efabf3a83e5336e44d2ba0577bd31c08c15937756b833164bbe36b345dc564e"}, "34430041-6a29-48a3-97f3-91a857b2cd94": {"doc_hash": "53970df51bb4144fde420f64dfe166bda2505f5c6d64332239cfd4af562fede4"}, "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f": {"doc_hash": "1f3d15167424c26c1e2228b8a98ae03e5f28f92ab28e6ead57008e8e7919579d"}, "99f2d48e-e376-49ca-99aa-54dd28e22a0c": {"doc_hash": "a31b82f1900ffcbd97f9e51ef790a8ca355f33c3afefa42fb452a2448df12d5a"}, "cddd2d64-752c-40b7-a0ce-d246925980f0": {"doc_hash": "8919f916647ae0da0103871ee330495c6fdbe6d1f8040277a26926c6910391ed"}, "6e4f88cb-2d81-47e7-9298-846df9e28302": {"doc_hash": "f300fbbfd83ec321a57aaf22ef62c252bf78a0f59101635b607563835944e9fe"}, "b491bdd6-cec2-4090-9899-ca17e6409b0a": {"doc_hash": "0256ff6916a8fba976adc7e9ad5e12e5f3c1367a816f6011732676c90b2a5991"}, "7856a549-5a6a-4be7-86a4-40d5a7516dd6": {"doc_hash": "6110665b9a1b4f7b852b226effa7d3165e8888e951ee3d4869056354dc4c3b6c"}, "2f0e6c81-de16-4476-9c9f-f34f8f79a94c": {"doc_hash": "b79e220385bc794f0399729a9440fa09eada01880f3822bde99238e893222537", "ref_doc_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d"}, "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795": {"doc_hash": "d88a4ef8a27e0d98be207b946cad7c30804930a629abcded3175ba0c354f56dd", "ref_doc_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d"}, "fa17ec7b-d8e2-4cf4-8a32-cf9f8516e6d8": {"doc_hash": "cf445ac918321f393e4c9fcb6eb45180499ae06f84beb6ec79d90714df7d2551", "ref_doc_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d"}, "dc190888-e184-4597-a0e1-651e2bdff8e6": {"doc_hash": "5d3559593ba6b9262f5d13bdaeca8e2199a106b0a215b7fde00aef7172138437", "ref_doc_id": "34430041-6a29-48a3-97f3-91a857b2cd94"}, "78082cf4-0b32-4753-b068-d2d234bfdd86": {"doc_hash": "da3185e3530269bcd3b2aaf9826d6027383a59ded9407c3e119f409dd70bfd98", "ref_doc_id": "34430041-6a29-48a3-97f3-91a857b2cd94"}, "7c37e502-64b8-49fe-92f9-40d5ef17d45d": {"doc_hash": "3c8b7bb4cf808a9643ec0ec1a37396b394e3a530633f8a7d35a8921dc6331942", "ref_doc_id": "34430041-6a29-48a3-97f3-91a857b2cd94"}, "bc7e6bfa-3fc4-483a-a557-8294b9993722": {"doc_hash": "2c8681443e43d669a11bd0dc3a776bb98323e418b9f445d7908f63096f6289d4", "ref_doc_id": "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f"}, "2f745255-5d4b-4165-8a0a-efb07f30718e": {"doc_hash": "5ce04f2628c2275777398cd979934e5d1933367d3813776150e615b8263c4b0d", "ref_doc_id": "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f"}, "77856852-9c5b-4e9f-a54f-34b84ceebfa8": {"doc_hash": "9a20968b3d2837233a45f5f35118b6c0e3c63a58c579f831889a4b88285313a3", "ref_doc_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c"}, "48603a9b-5a3c-4d97-83d8-28572a7c91cd": {"doc_hash": "6bc2eeb49a9ceed1677f760a334210311e34b48516f006d0257fb1ccc6666a7f", "ref_doc_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c"}, "68a7fb90-404d-4ee9-8c0d-471d48923d90": {"doc_hash": "5c81842df57b9469560a55f5662f136abc3a539d74006090c16b8064822d78b2", "ref_doc_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c"}, "1bea2c0a-5bde-4ed2-a223-24be16bf9c40": {"doc_hash": "ff7271af215ee2f70480ae74892931d9f7ceb87e49e659560724b447d6bfe739", "ref_doc_id": "cddd2d64-752c-40b7-a0ce-d246925980f0"}, "c4b26b15-26b4-435b-adc4-0a8705955e70": {"doc_hash": "80ef0db31fcbec2d8bf8c4477d612b695b7f8c5368eb65fc0b13ca94ff45ef35", "ref_doc_id": "cddd2d64-752c-40b7-a0ce-d246925980f0"}, "6d1dfa99-7877-4667-a50a-30be95634780": {"doc_hash": "9dc25c1c5014dbc363536f00b8ec99ceb830526457f61be0b8f9e740e2512655", "ref_doc_id": "cddd2d64-752c-40b7-a0ce-d246925980f0"}, "eb72f14d-98ab-4fbe-8648-a3d51e39ec61": {"doc_hash": "b23902594b20ec5a5109d860bf672eaa03451cb63a359c0b3ebf78184bc7c34e", "ref_doc_id": "6e4f88cb-2d81-47e7-9298-846df9e28302"}, "8a582964-0e3f-437b-a88e-525f4cd0ba9c": {"doc_hash": "713593fead2eb1fe09cf069dadabdb910952faf1c0cd2610d7c85c928c020b9c", "ref_doc_id": "b491bdd6-cec2-4090-9899-ca17e6409b0a"}, "5ac4d6e0-9302-4369-b395-36ea7a28f2a9": {"doc_hash": "52a7a400de7921200103878dd2832039242976a6687bd917c379efccfbef1253", "ref_doc_id": "7856a549-5a6a-4be7-86a4-40d5a7516dd6"}}, "docstore/data": {"2f0e6c81-de16-4476-9c9f-f34f8f79a94c": {"__data__": {"id_": "2f0e6c81-de16-4476-9c9f-f34f8f79a94c", "embedding": null, "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5efabf3a83e5336e44d2ba0577bd31c08c15937756b833164bbe36b345dc564e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795", "node_type": "1", "metadata": {}, "hash": "7913f4e48c77b71c792dd4335acf7a45a0cd74327c02c7d7852fb18e69026447", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "BERT Rediscovers the Classical NLP Pipeline\nIan Tenney1 Dipanjan Das1 Ellie Pavlick1,2\n1Google Research 2Brown University\n{iftenney,dipanjand,epavlick}@google.com\nAbstract\nPre-trained text encoders have rapidly ad-\nvanced the state of the art on many NLP\ntasks. We focus on one such model, BERT,\nand aim to quantify where linguistic informa-\ntion is captured within the network. We \ufb01nd\nthat the model represents the steps of the tra-\nditional NLP pipeline in an interpretable and\nlocalizable way, and that the regions respon-\nsible for each step appear in the expected se-\nquence: POS tagging, parsing, NER, semantic\nroles, then coreference. Qualitative analysis\nreveals that the model can and often does ad-\njust this pipeline dynamically, revising lower-\nlevel decisions on the basis of disambiguating\ninformation from higher-level representations.\n1 Introduction\nPre-trained sentence encoders such as ELMo (Pe-\nters et al., 2018a) and BERT (Devlin et al., 2018)\nhave rapidly improved the state of the art on many\nNLP tasks, and seem poised to displace both static\nword embeddings (Mikolov et al., 2013) and dis-\ncrete pipelines (Manning et al., 2014) as the basis\nfor natural language processing systems. While\nthis has been a boon for performance, it has come\nat the cost of interpretability, and it remains un-\nclear whether such models are in fact learning\nthe kind of abstractions that we intuitively believe\nare important for representing natural language,\nor simply modeling complex co-occurrence statis-\ntics.\nA wave of recent work has begun to \u201cprobe\u201d\nstate-of-the-art models to understand whether they\nare representing language in a satisfying way.\nMuch of this work is behavior-based, designing\ncontrolled test sets and analyzing errors in order\nto reverse-engineer the types of abstractions the\nmodel may or may not be representing (e.g. Con-\nneau et al., 2018; Marvin and Linzen, 2018; Poliak\net al., 2018).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795": {"__data__": {"id_": "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795", "embedding": null, "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5efabf3a83e5336e44d2ba0577bd31c08c15937756b833164bbe36b345dc564e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f0e6c81-de16-4476-9c9f-f34f8f79a94c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "b79e220385bc794f0399729a9440fa09eada01880f3822bde99238e893222537", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa17ec7b-d8e2-4cf4-8a32-cf9f8516e6d8", "node_type": "1", "metadata": {}, "hash": "fbdc7a7083adbff3a46d6015ca4e916c7ffb07ab930d6227cc1627bde2fdcf3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Con-\nneau et al., 2018; Marvin and Linzen, 2018; Poliak\net al., 2018). Parallel efforts inspect the structure\nof the network directly, to assess whether there\nexist localizable regions associated with distinct\ntypes of linguistic decisions. Such work has pro-\nduced evidence that deep language models can en-\ncode a range of syntactic and semantic informa-\ntion (e.g. Shi et al., 2016; Belinkov, 2018; Tenney\net al., 2019), and that more complex structures are\ndeveloped hierarchically in the higher layers of the\nmodel (Peters et al., 2018b; Blevins et al., 2018).\nWe build on this latter line of work, focusing\non the BERT model (Devlin et al., 2018), and use\na suite of probing tasks (Tenney et al., 2019) de-\nrived from the traditional NLP pipeline to quantify\nwhere speci\ufb01c types of linguistic information are\nencoded. Building on observations (Peters et al.,\n2018b) that lower layers of a language model en-\ncode more local syntax while higher layers capture\nmore complex semantics, we present two novel\ncontributions. First, we present an analysis that\nspans the common components of a traditional\nNLP pipeline. We show that the order in which\nspeci\ufb01c abstractions are encoded re\ufb02ects the tradi-\ntional hierarchy of these tasks. Second, we quali-\ntatively analyze how individual sentences are pro-\ncessed by the BERT network, layer-by-layer. We\nshow that while the pipeline order holds in ag-\ngregate, the model can allow individual decisions\nto depend on each other in arbitrary ways, de-\nferring ambiguous decisions or revising incorrect\nones based on higher-level information.\n2 Model\nEdge Probing. Our experiments are based on\nthe \u201cedge probing\u201d approach of Tenney et al.\n(2019), which aims to measure how well infor-\nmation about linguistic structure can be extracted\nfrom a pre-trained encoder.", "mimetype": "text/plain", "start_char_idx": 1852, "end_char_idx": 3659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa17ec7b-d8e2-4cf4-8a32-cf9f8516e6d8": {"__data__": {"id_": "fa17ec7b-d8e2-4cf4-8a32-cf9f8516e6d8", "embedding": null, "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3655a5b6-96d3-4a85-bccc-610d5effef2d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5efabf3a83e5336e44d2ba0577bd31c08c15937756b833164bbe36b345dc564e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795", "node_type": "1", "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "d88a4ef8a27e0d98be207b946cad7c30804930a629abcded3175ba0c354f56dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 Model\nEdge Probing. Our experiments are based on\nthe \u201cedge probing\u201d approach of Tenney et al.\n(2019), which aims to measure how well infor-\nmation about linguistic structure can be extracted\nfrom a pre-trained encoder. Edge probing decom-\nposes structured-prediction tasks into a common\nformat, where a probing classi\ufb01er receives spans\narXiv:1905.05950v1  [cs.CL]  15 May 2019", "mimetype": "text/plain", "start_char_idx": 3439, "end_char_idx": 3817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc190888-e184-4597-a0e1-651e2bdff8e6": {"__data__": {"id_": "dc190888-e184-4597-a0e1-651e2bdff8e6", "embedding": null, "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34430041-6a29-48a3-97f3-91a857b2cd94", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "53970df51bb4144fde420f64dfe166bda2505f5c6d64332239cfd4af562fede4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78082cf4-0b32-4753-b068-d2d234bfdd86", "node_type": "1", "metadata": {}, "hash": "70a58f6bdaf232433fa31f73786d2806544d8f68490d961c851c0ec0cc0a721d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "s1 = [ i1,j1) and (optionally) s2 = [ i2,j2) and\nmust predict a label such as a constituent or rela-\ntion type.1 The probing classi\ufb01er has access only\nto the per-token contextual vectors within the tar-\nget spans, and so must rely on the encoder to pro-\nvide information about the relation between these\nspans and their role in the sentence.\nWe use eight labeling tasks from the edge\nprobing suite: part-of-speech (POS), constituents\n(Consts.), dependencies (Deps.), entities, semantic\nrole labeling (SRL), coreference (Coref.), seman-\ntic proto-roles (SPR; Reisinger et al., 2015), and\nrelation classi\ufb01cation (SemEval). These tasks are\nderived from standard benchmark datasets 2 , and\nevaluated with a common metric\u2013micro-averaged\nF1\u2013to facilitate comparison across tasks.\nBERT. The BERT model (Devlin et al., 2018)\nhas shown state-of-the-art performance on many\ntasks, and its deep Transformer architecture\n(Vaswani et al., 2017) is typical of many recent\nmodels (e.g. Radford et al., 2018, 2019; Liu et al.,\n2019). We focus on the stock BERT models\n(base and large, uncased), which are trained with\na multi-task objective (masked language modeling\nand next-sentence prediction) over a 3.3B word\nEnglish corpus. Since we want to understand how\nthe network is structured as a result of pretrain-\ning, we follow Tenney et al. (2019) (departing\nfrom standard BERT usage) and freeze the en-\ncoder weights. This prevents the encoder from re-\narranging its internal representations to better suit\nthe probing task.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78082cf4-0b32-4753-b068-d2d234bfdd86": {"__data__": {"id_": "78082cf4-0b32-4753-b068-d2d234bfdd86", "embedding": null, "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34430041-6a29-48a3-97f3-91a857b2cd94", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "53970df51bb4144fde420f64dfe166bda2505f5c6d64332239cfd4af562fede4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc190888-e184-4597-a0e1-651e2bdff8e6", "node_type": "1", "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5d3559593ba6b9262f5d13bdaeca8e2199a106b0a215b7fde00aef7172138437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c37e502-64b8-49fe-92f9-40d5ef17d45d", "node_type": "1", "metadata": {}, "hash": "394c9a33ce67b46d917226d98904ae32abb6b93861788de769e1d52237d97743", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) (departing\nfrom standard BERT usage) and freeze the en-\ncoder weights. This prevents the encoder from re-\narranging its internal representations to better suit\nthe probing task.\nGiven input tokens T = [ t0,t1,...,t n],\na deep encoder produces a set of layer ac-\ntivations H(0),H(1),...,H (L), where H(\u2113) =\n[h(\u2113)\n0 ,h(\u2113)\n1 ,..., h(\u2113)\nn ] are the activation vectors of\nthe \u2113th layer and H(0) corresponds to the non-\ncontextual word(piece) embeddings. We use a\nweighted sum ( \u00a73.1) to pool these a single set of\nvectors H = [h0,h1,..., hn], and train a probing\nclassi\ufb01er P\u03c4 for each task using the architecture\nand procedure of Tenney et al. (2019).\n1For single-span tasks (POS, entities, and constituents),\ns2 is not used. For POS, s1 = [i, i+ 1)is a single token.\n2We use the authors\u2019 code from https://github.\ncom/jsalt18-sentence-repl/jiant. Dependen-\ncies is the English Web Treebank (Silveira et al., 2014), SPR\nis the SPR1 dataset of (Teichert et al., 2017), and relations\nis SemEval 2010 Task 8 (Hendrickx et al., 2009). All other\ntasks are from OntoNotes 5.0 (Weischedel et al., 2013).\n3 Metrics\nWe de\ufb01ne two. The \ufb01rst, scalar mixing weights\n(\u00a73.1) tell us which layers, in combination, are\nmost relevant when a probing classi\ufb01er has ac-\ncess to the whole BERT model. The second, cu-\nmulative scoring (\u00a73.2) tells us how much higher\nwe can score on a probing task with the introduc-\ntion of each layer. These metrics provide com-\nplementary views on what is happening inside the\nmodel.", "mimetype": "text/plain", "start_char_idx": 1326, "end_char_idx": 2824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c37e502-64b8-49fe-92f9-40d5ef17d45d": {"__data__": {"id_": "7c37e502-64b8-49fe-92f9-40d5ef17d45d", "embedding": null, "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34430041-6a29-48a3-97f3-91a857b2cd94", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "53970df51bb4144fde420f64dfe166bda2505f5c6d64332239cfd4af562fede4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78082cf4-0b32-4753-b068-d2d234bfdd86", "node_type": "1", "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "da3185e3530269bcd3b2aaf9826d6027383a59ded9407c3e119f409dd70bfd98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second, cu-\nmulative scoring (\u00a73.2) tells us how much higher\nwe can score on a probing task with the introduc-\ntion of each layer. These metrics provide com-\nplementary views on what is happening inside the\nmodel. Mixing weights are learned solely from the\ntraining data\u2013they tell us which layers the probing\nmodel \ufb01nds most useful. In contrast, cumulative\nscoring is derived entirely from an evaluation set,\nand tell us how many layers are needed for a cor-\nrect prediction.\n3.1 Scalar Mixing Weights\nTo pool across layers, we use the scalar mixing\ntechnique introduced by the ELMo model. Fol-\nlowing Peters et al. (2018a), for each task we intro-\nduce scalar parameters \u03b3\u03c4 and a(0)\n\u03c4 ,a(1)\n\u03c4 ,...,a (L)\n\u03c4 ,\nand let:\nhi,\u03c4 = \u03b3\u03c4\nL\u2211\n\u2113=0\ns(\u2113)\n\u03c4 h(\u2113)\ni (1)\nwhere s\u03c4 = softmax(a\u03c4). We learn these weights\njointly with the probing classi\ufb01er P\u03c4, in order to\nallow it to extract information from the many lay-\ners of an encoder without adding a large number\nof parameters. After the probing model is trained,\nwe extract the learned coef\ufb01cients in order to es-\ntimate the contribution of different layers to that\nparticular task. We interpret higher weights as ev-\nidence that the corresponding layer contains more\ninformation related to that particular task.\nCenter-of-Gravity. As a summary statistic, we\nde\ufb01ne the mixing weight center of gravity as:\n\u00afEs[\u2113] =\nL\u2211\n\u2113=0\n\u2113\u00b7s(\u2113)\n\u03c4 (2)\nThis re\ufb02ects the average layer attended to for each\ntask; intuitively, we can interpret a higher value to\nmean that the information needed for that task is\ncaptured by higher layers.\n3.2 Cumulative Scoring\nWe would like to estimate at which layer in the\nencoder a target (s1,s2,label) can be correctly\npredicted. Mixing weights cannot tell us this di-\nrectly, because they are learned as parameters and", "mimetype": "text/plain", "start_char_idx": 2607, "end_char_idx": 4385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc7e6bfa-3fc4-483a-a557-8294b9993722": {"__data__": {"id_": "bc7e6bfa-3fc4-483a-a557-8294b9993722", "embedding": null, "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "1f3d15167424c26c1e2228b8a98ae03e5f28f92ab28e6ead57008e8e7919579d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f745255-5d4b-4165-8a0a-efb07f30718e", "node_type": "1", "metadata": {}, "hash": "d751539b396abde50f1a826653c6bcb75478516468253ee8c00def5d0f6880f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1: Summary statistics on BERT-large. Columns\non left show F1 dev-set scores for the baseline ( P(0)\n\u03c4 )\nand full-model (P(L)\n\u03c4 ) probes. Dark (blue) are the mix-\ning weight center of gravity (Eq. 2); light (purple) are\nthe expected layer from the cumulative scores (Eq. 4).\ndo not correspond to a distribution over data. A\nnaive classi\ufb01er at a single layer cannot either, be-\ncause information about a particular span may be\nspread out across several layers, and as observed\nin Peters et al. (2018b) the encoder may choose to\ndiscard information at higher layers.\nTo address this, we train a series of classi\ufb01ers\n{P(\u2113)\n\u03c4 }\u2113 which use scalar mixing (Eq. 1) to attend\nto layer \u2113as well asall previouslayers. P(0)\n\u03c4 corre-\nsponds to a non-contextual baseline that uses only\na bag of word(piece) embeddings, while P(L)\n\u03c4 =\nP\u03c4 corresponds to probing all layers of the BERT\nmodel.\nThese classi\ufb01ers are cumulative, in the sense\nthat P(\u2113+1)\n\u03c4 has a similar number of parameters but\nwith access to strictly more information thanP(\u2113)\n\u03c4 ,\nand we see intuitively that performance (F1 score)\ngenerally increases as more layers are added.3 We\ncan then compute a differential score, which mea-\nsures how much better we do on the probing task\nif we observe one additional encoder layer \u2113:\n\u2206(\u2113)\n\u03c4 = Score(P(\u2113)\n\u03c4 ) \u2212Score(P(\u2113\u22121)\n\u03c4 ) (3)\nExpected Layer. Again, we compute a (pseudo)\nexpectation over the differential scores as a sum-\nmary statistic.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f745255-5d4b-4165-8a0a-efb07f30718e": {"__data__": {"id_": "2f745255-5d4b-4165-8a0a-efb07f30718e", "embedding": null, "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "1f3d15167424c26c1e2228b8a98ae03e5f28f92ab28e6ead57008e8e7919579d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc7e6bfa-3fc4-483a-a557-8294b9993722", "node_type": "1", "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "2c8681443e43d669a11bd0dc3a776bb98323e418b9f445d7908f63096f6289d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, we compute a (pseudo)\nexpectation over the differential scores as a sum-\nmary statistic. To focus on non-trivial examples,\nwe normalize over the contextual layers \u2113> 0:\n\u00afE\u2206[\u2113] =\n\u2211L\n\u2113=1 \u2113\u00b7\u2206(\u2113)\n\u03c4\n\u2211L\n\u2113=1 \u2206(\u2113)\n\u03c4\n(4)\n3Note that if a new layer provides distracting features, the\nprobing model can over\ufb01t and performance can drop. We see\nthis in particular in the last 1-2 layers (Figure 2).\nFigure 2: Layer-wise metrics on BERT-large. Solid\n(blue) are mixing weights ( \u00a73.1); outlined (purple) are\ndifferential scores \u2206(\u2113)\n\u03c4 (\u00a73.2), normalized for each\ntask. Horizontal axis is encoder layer.\nThis can be thought of as, approximately, the ex-\npected layer at which an example can be correctly\nlabeled, assuming that example could not be re-\nsolved by the non-contextual baseline P(0)\n\u03c4 .\n4 Results\nFigure 1 reports summary statistics; Figure 2 re-\nports per-layer metrics. We also report K(\u22c6) =\nKL(\u22c6||Uniform) to estimate how non-uniform\neach statistic (\u22c6= s\u03c4,\u2206\u03c4) is for each task.\nLinguistic Patterns. We observe a consistent\ntrend across both of our metrics, with the tasks\nencoded in a natural progression: POS tags pro-\ncessed earliest, followed by constituents, depen-\ndencies, semantic roles, and coreference. That is,\nit appears that basic syntactic information appears\nearlier in the network, while high-level semantic\ninformation appears at higher layers. We note that", "mimetype": "text/plain", "start_char_idx": 1342, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77856852-9c5b-4e9f-a54f-34b84ceebfa8": {"__data__": {"id_": "77856852-9c5b-4e9f-a54f-34b84ceebfa8", "embedding": null, "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a31b82f1900ffcbd97f9e51ef790a8ca355f33c3afefa42fb452a2448df12d5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48603a9b-5a3c-4d97-83d8-28572a7c91cd", "node_type": "1", "metadata": {}, "hash": "efee19527b57ffc8a92b1f4b568b7befd822e5161e7b6e3c40b11d326a7b99b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "this \ufb01nding is consistent with initial observations\nby Peters et al. (2018b), which found that con-\nstituents are represented earlier than coreference.\nIn addition, we observe that in general, syntactic\ninformation is more localizable, with weights re-\nlated to syntactic tasks tending to be more \u201cspiky\u201d\n(high K(s) and K(\u2206)), while information related\nto semantic tasks is generally spread across the en-\ntire network. For example, we \ufb01nd that for se-\nmantic relations and proto-roles (SPR), the mix-\ning weights are close to uniform, and that nontriv-\nial examples for these tasks are resolved gradually\nacross nearly all layers. For entity labeling many\nexamples are resolved in layer 1, but with a long\ntail thereafter, and only a weak concentration of\nmixing weights in high layers. Further study is\nneeded to determine whether this is because BERT\nhas dif\ufb01culty representing the correct abstraction\nfor these tasks, or because semantic information is\ninherently harder to localize.\nComparison of Metrics. For many tasks, we\n\ufb01nd that the differential scores are highest in the\n\ufb01rst few layers of the model (layers 1-7 for BERT-\nlarge), i.e. most examples can be correctly classi-\n\ufb01ed very early on. We attribute this to the avail-\nability of heuristic shortcuts: while challenging\nexamples may not be resolved until much later,\nmany cases can be guessed from shallow statistics.\nConversely, we observe that the learned mixing\nweights are concentrated much later, layers 9-20\nfor BERT-large.4 We observe\u2013particularly when\nweights are highly concentrated\u2013that the highest\nweights are found on or just after the highest lay-\ners which give an improvement \u2206(\u2113)\n\u03c4 in F1 score.\nPer-Example Analysis. We explore, qualita-\ntively, how beliefs about the structure of individ-\nual sentences develop over the layers of the BERT\nnetwork. For this, we compile the predictions of\nthe per-layer classi\ufb01ers P(\u2113)\n\u03c4 for different anno-\ntations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48603a9b-5a3c-4d97-83d8-28572a7c91cd": {"__data__": {"id_": "48603a9b-5a3c-4d97-83d8-28572a7c91cd", "embedding": null, "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a31b82f1900ffcbd97f9e51ef790a8ca355f33c3afefa42fb452a2448df12d5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77856852-9c5b-4e9f-a54f-34b84ceebfa8", "node_type": "1", "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "9a20968b3d2837233a45f5f35118b6c0e3c63a58c579f831889a4b88285313a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68a7fb90-404d-4ee9-8c0d-471d48923d90", "node_type": "1", "metadata": {}, "hash": "30d9531ec54ba7da58742e8d9c3eeaa2bef559f8ce20163991ca54a4c4012927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We explore, qualita-\ntively, how beliefs about the structure of individ-\nual sentences develop over the layers of the BERT\nnetwork. For this, we compile the predictions of\nthe per-layer classi\ufb01ers P(\u2113)\n\u03c4 for different anno-\ntations. Figure 3 shows examples selected from\nthe OntoNotes development set, in which the same\nsentence is annotated for multiple tasks.\nWe \ufb01nd that while the pipeline order holds on\naverage (Figure 2), for individual examples the\nmodel is free to, and often does, choose a different\norder. In the \ufb01rst example, the model originally\n(incorrectly) assumes that \u201cToronto\u201d refers to the\n4We \ufb01nd that in the smaller BERT-base model, the\nweights are concentrated at roughly the same layers relative\nto the top of the model. See Supplementary Material.\n(a) he smoked toronto in the playoffs with six hits, ...\n(b) china today blacked out a cnn interview that was ...\nFigure 3: Probing classi\ufb01er predictions across lay-\ners of BERT-base. Blue is the correct label; or-\nange is the incorrect label with highest average score\nover layers. Bar heights are (normalized) probabilities\nP(\u2113)\n\u03c4 (label|s1,s2). Only select tasks shown for space.\ncity, tagging it as a GPE. However, after determin-\ning that \u201cToronto\u201d is the thing getting \u201csmoked\u201d\n(ARG1), this decision is revised and it is tagged as\nORG (i.e. the sports team). In the second exam-\nple, the model initially tags \u201ctoday\u201d as a common\nnoun, date, and temporal modi\ufb01er ( ARGM-TMP).\nHowever, this phrase is ambiguous, and it later\nreinterprets \u201cchina today\u201d as a proper noun (i.e.\na TV network) and updates its beliefs about the\nentity type and the semantic role accordingly. See\nSupplementary Material for additional examples.\n5 Conclusion\nWe employ the edge probing task suite to explore\nhow the different layers of the BERT network can\nresolve syntactic and semantic structure within a\nsentence.", "mimetype": "text/plain", "start_char_idx": 1699, "end_char_idx": 3567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68a7fb90-404d-4ee9-8c0d-471d48923d90": {"__data__": {"id_": "68a7fb90-404d-4ee9-8c0d-471d48923d90", "embedding": null, "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99f2d48e-e376-49ca-99aa-54dd28e22a0c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a31b82f1900ffcbd97f9e51ef790a8ca355f33c3afefa42fb452a2448df12d5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48603a9b-5a3c-4d97-83d8-28572a7c91cd", "node_type": "1", "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "6bc2eeb49a9ceed1677f760a334210311e34b48516f006d0257fb1ccc6666a7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See\nSupplementary Material for additional examples.\n5 Conclusion\nWe employ the edge probing task suite to explore\nhow the different layers of the BERT network can\nresolve syntactic and semantic structure within a\nsentence. We present two complementary mea-\nsurements: scalar mixing weights, learned from a\ntraining corpus, and cumulative scoring, measured\non a development set, and show that a consistent\nordering emerges. We \ufb01nd that while this tradi-\ntional pipeline order holds in the aggregate, on in-\ndividual examples the network can resolve out-of-\norder, using high-level information like predicate-\nargument relations to help disambiguate low-level\ndecisions like part-of-speech. This provides new\nevidence corroborating that deep language mod-", "mimetype": "text/plain", "start_char_idx": 3345, "end_char_idx": 4098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bea2c0a-5bde-4ed2-a223-24be16bf9c40": {"__data__": {"id_": "1bea2c0a-5bde-4ed2-a223-24be16bf9c40", "embedding": null, "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cddd2d64-752c-40b7-a0ce-d246925980f0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "8919f916647ae0da0103871ee330495c6fdbe6d1f8040277a26926c6910391ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4b26b15-26b4-435b-adc4-0a8705955e70", "node_type": "1", "metadata": {}, "hash": "58d59bf3153511b8c42a5aad0f21b99569d13d6c3ac7f93831b7ffa45b7940d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "els can represent the types of syntactic and se-\nmantic abstractions traditionally believed neces-\nsary for language processing, and moreover that\nthey can model complex interactions between dif-\nferent levels of hierarchical information.\nReferences\nYonatan Belinkov. 2018. On internal language repre-\nsentations in deep learning: An analysis of machine\ntranslation and speech recognition. Ph.D. thesis,\nMassachusetts Institute of Technology.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep RNNs encode soft hierarchical syntax.\nIn Proceedings of ACL.\nAlexis Conneau, Germ \u00b4an Kruszewski, Guillaume\nLample, Lo \u00a8\u0131c Barrault, and Marco Baroni. 2018.\nWhat you can cram into a single $& #* vector: Prob-\ning sentence embeddings for linguistic properties.\nIn Proceedings of ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint 1810.04805.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid \u00b4O S \u00b4eaghdha, Sebastian\nPad\u00b4o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2009. Semeval-2010 task 8:\nMulti-way classi\ufb01cation of semantic relations be-\ntween pairs of nominals. In Proceedings of\nthe Workshop on Semantic Evaluations: Recent\nAchievements and Future Directions.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint\n1901.11504.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of ACL: System\nDemonstrations.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4b26b15-26b4-435b-adc4-0a8705955e70": {"__data__": {"id_": "c4b26b15-26b4-435b-adc4-0a8705955e70", "embedding": null, "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cddd2d64-752c-40b7-a0ce-d246925980f0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "8919f916647ae0da0103871ee330495c6fdbe6d1f8040277a26926c6910391ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bea2c0a-5bde-4ed2-a223-24be16bf9c40", "node_type": "1", "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "ff7271af215ee2f70480ae74892931d9f7ceb87e49e659560724b447d6bfe739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d1dfa99-7877-4667-a50a-30be95634780", "node_type": "1", "metadata": {}, "hash": "6d562c2180aad4a4b97449ce3e08b635d320e1825e238c5a9840735e02edcefd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of ACL: System\nDemonstrations.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of EMNLP.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Proceedings of NIPS.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of NAACL.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of EMNLP.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger,\nJ. Edward Hu, Ellie Pavlick, Aaron Steven White,\nand Benjamin Van Durme. 2018. Collecting di-\nverse natural language inference problems for sen-\ntence representation evaluation. In Proceedings of\nEMNLP.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training.\nhttps://blog.openai.com/language-unsupervised.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nhttps://blog.openai.com/better-language-models.\nDrew Reisinger, Rachel Rudinger, Francis Ferraro,\nCraig Harman, Kyle Rawlins, and Benjamin Van\nDurme. 2015. Semantic proto-roles. Transactions\nof the Association of Computational Linguistics.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of EMNLP.", "mimetype": "text/plain", "start_char_idx": 1591, "end_char_idx": 3303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d1dfa99-7877-4667-a50a-30be95634780": {"__data__": {"id_": "6d1dfa99-7877-4667-a50a-30be95634780", "embedding": null, "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cddd2d64-752c-40b7-a0ce-d246925980f0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "8919f916647ae0da0103871ee330495c6fdbe6d1f8040277a26926c6910391ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4b26b15-26b4-435b-adc4-0a8705955e70", "node_type": "1", "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "80ef0db31fcbec2d8bf8c4477d612b695b7f8c5368eb65fc0b13ca94ff45ef35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2015. Semantic proto-roles. Transactions\nof the Association of Computational Linguistics.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of EMNLP.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Christopher D. Manning. 2014. A\ngold standard dependency corpus for English. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation.\nAdam Teichert, Adam Poliak, Benjamin Van Durme,\nand Matthew Gormley. 2017. Semantic proto-role\nlabeling. In Proceedings of AAAI.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw,\nNianwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. OntoNotes release\n5.0 LDC2013T19. Linguistic Data Consortium,\nPhiladelphia, PA.", "mimetype": "text/plain", "start_char_idx": 3091, "end_char_idx": 4462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb72f14d-98ab-4fbe-8648-a3d51e39ec61": {"__data__": {"id_": "eb72f14d-98ab-4fbe-8648-a3d51e39ec61", "embedding": null, "metadata": {"page_label": "6", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e4f88cb-2d81-47e7-9298-846df9e28302", "node_type": "4", "metadata": {"page_label": "6", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "f300fbbfd83ec321a57aaf22ef62c252bf78a0f59101635b607563835944e9fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 Supplemental Material\n6.1 Comparison of Different Encoders\nWe reproduce Figure 1 and Figure 2 (which depict\nmetrics on BERT-large) from the main paper be-\nlow, and show analogous plots for the BERT-base\nmodels. We observe that the most important layers\nfor a given task appear in roughly the samerelative\nposition on both the 24-layer BERT-large and 12-\nlayer BERT-base models, and that tasks generally\nappear in the same order.\n6.2 Additional Examples\nWe provide additional examples in the style of\nFigure 3, which illustrate sequential decisions in\nthe layers of the BERT-base model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a582964-0e3f-437b-a88e-525f4cd0ba9c": {"__data__": {"id_": "8a582964-0e3f-437b-a88e-525f4cd0ba9c", "embedding": null, "metadata": {"page_label": "7", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b491bdd6-cec2-4090-9899-ca17e6409b0a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "0256ff6916a8fba976adc7e9ad5e12e5f3c1367a816f6011732676c90b2a5991", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) BERT-base\n (b) BERT-large\nFigure S.1: Summary statistics on BERT-base (left) and BERT-large (right). Columns on left show F1 dev-set\nscores for the baseline (P(0)\n\u03c4 ) and full-model (P(L)\n\u03c4 ) probes. Dark (blue) are the mixing weight center of gravity;\nlight (purple) are the expected layer from the cumulative scores.\n(a) BERT-base\n (b) BERT-large\nFigure S.2: Layer-wise metrics on BERT-base (left) and BERT-large (right). Solid (blue) are mixing weights;\noutlined (purple) are differential scores \u2206(\u2113)\n\u03c4 , normalized for each task. Horizontal axis is encoder layer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ac4d6e0-9302-4369-b395-36ea7a28f2a9": {"__data__": {"id_": "5ac4d6e0-9302-4369-b395-36ea7a28f2a9", "embedding": null, "metadata": {"page_label": "8", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7856a549-5a6a-4be7-86a4-40d5a7516dd6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "6110665b9a1b4f7b852b226effa7d3165e8888e951ee3d4869056354dc4c3b6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure S.3: Trace of selected annotations that intersect\nthe token \u201cbasque\u201d in the above sentence. We see the\nmodel recognize this as part of a proper noun ( NNP)\nin layer 2, which leads it to revise its hypothesis about\nthe constituent \u201cpetro basque\u201d from an ordinary noun\nphrase (NP) to a nominal mention (NML) in layers 3-4.\nWe also see that from layer 3 onwards, the model rec-\nognizes \u201cpetro basque\u201d as either an organization (ORG)\nor a national or religious group ( NORP), but does not\nstrongly disambiguate between the two.\nFigure S.4: Trace of selected annotations that intersect\nthe second \u201ctoday\u201d in the above sentence. The odel ini-\ntially believes this to be a date and a common noun, but\nby layer 4 realizes that this is the TV show (entity tag\nWORK OF ART) and subsequently revises its hypothe-\nses about the constituent type and part-of-speech.\nFigure S.5: Trace of selected coreference annotations\non the above sentence. Not shown are two coreference\nedges that the model has correctly resolved at layer\n0 (guessing from embeddings alone): \u201chim\u201d and \u201cthe\nhurt man\u201d are coreferent, as are \u201che\u201d and \u201che\u201d. We see\nthat the remaining edges, between non-coreferent men-\ntions, are resolved in several stages.\nFigure S.6: Trace of selected coreference and SRL an-\nnotations on the above sentence. The model resolves\nthe semantic role (purpose, ARGM-PRP) of the phrase\n\u201cto help him\u201d in layers 5-7, then quickly resolves at\nlayer 8 that \u201chim\u201d and \u201che\u201d (the agent of \u201cstop\u201d) are\nnot coreferent. Also shown is the correct prediction\nthat \u201chim\u201d is the recipient (ARG1, patient) of \u201chelp\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"3655a5b6-96d3-4a85-bccc-610d5effef2d": {"node_ids": ["2f0e6c81-de16-4476-9c9f-f34f8f79a94c", "a0ad16b6-1ac9-4e76-81ae-b6c20cc8b795", "fa17ec7b-d8e2-4cf4-8a32-cf9f8516e6d8"], "metadata": {"page_label": "1", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "34430041-6a29-48a3-97f3-91a857b2cd94": {"node_ids": ["dc190888-e184-4597-a0e1-651e2bdff8e6", "78082cf4-0b32-4753-b068-d2d234bfdd86", "7c37e502-64b8-49fe-92f9-40d5ef17d45d"], "metadata": {"page_label": "2", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "c10fe71c-6b20-4b6e-8841-3f48b2cf4c2f": {"node_ids": ["bc7e6bfa-3fc4-483a-a557-8294b9993722", "2f745255-5d4b-4165-8a0a-efb07f30718e"], "metadata": {"page_label": "3", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "99f2d48e-e376-49ca-99aa-54dd28e22a0c": {"node_ids": ["77856852-9c5b-4e9f-a54f-34b84ceebfa8", "48603a9b-5a3c-4d97-83d8-28572a7c91cd", "68a7fb90-404d-4ee9-8c0d-471d48923d90"], "metadata": {"page_label": "4", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "cddd2d64-752c-40b7-a0ce-d246925980f0": {"node_ids": ["1bea2c0a-5bde-4ed2-a223-24be16bf9c40", "c4b26b15-26b4-435b-adc4-0a8705955e70", "6d1dfa99-7877-4667-a50a-30be95634780"], "metadata": {"page_label": "5", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "6e4f88cb-2d81-47e7-9298-846df9e28302": {"node_ids": ["eb72f14d-98ab-4fbe-8648-a3d51e39ec61"], "metadata": {"page_label": "6", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "b491bdd6-cec2-4090-9899-ca17e6409b0a": {"node_ids": ["8a582964-0e3f-437b-a88e-525f4cd0ba9c"], "metadata": {"page_label": "7", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "7856a549-5a6a-4be7-86a4-40d5a7516dd6": {"node_ids": ["5ac4d6e0-9302-4369-b395-36ea7a28f2a9"], "metadata": {"page_label": "8", "file_name": "J2.pdf", "file_path": "uploaded_pdfs/J2.pdf", "file_type": "application/pdf", "file_size": 1013152, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}}}