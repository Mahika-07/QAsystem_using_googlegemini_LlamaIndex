{"docstore/metadata": {"7d6a3726-fe4f-44db-8599-37bc8f5c261d": {"doc_hash": "a45e8efe58616a905fc92a8431e4e755e3cd258893e0c2e0a3d86aa311763fe5"}, "e9e30e57-a68e-4e42-ad64-192b8bd29d8a": {"doc_hash": "aa5fdd9b0b04279466fb47e2314c4ab7b48df0a9b4be99010c70ea956a621c8a"}, "a03b08c2-81f0-49e1-b2f0-30c5f54ae816": {"doc_hash": "5a7bd5b0689e1af7970058e64ad7df6ea493f1d5c6480f6c89cb300a330697d9"}, "e06d64a7-ee72-41ba-b540-37b6012b609a": {"doc_hash": "e36d4f70e598805b688afc95d9170d8318d0a28d9bb1bc0dacfb0f5d814f8795"}, "260f8138-a208-4ccc-9a81-d2356237e0f4": {"doc_hash": "dc6ff39107002a9b3b9a71ec4826ebff0a278c0e47da38782068e78d43932167"}, "7e642a29-edd4-4d99-ad44-20fe8d791868": {"doc_hash": "777aa79895df50ac8b4b2d3073db88963a35209a1f29460ba562df36e96074b2"}, "321e2c6e-20c1-40e8-a35e-b7cdd536cd8a": {"doc_hash": "284b01c2a6a86820798d5127b4a4f13af35f753afcadc3e93c6249d6db0b48a2", "ref_doc_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d"}, "62e13e9a-a506-405d-944a-40d86e70c828": {"doc_hash": "03ac5d2f0e4c769005c734ab37f75781b56d703957dd29750e829e78fb1db779", "ref_doc_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d"}, "4e9dcfbc-166d-402b-ad8a-722fa9395e4f": {"doc_hash": "a557ec2b4aed25ae163c1778bf7a2c3c4e06b4eab00e9484a3c11e1923c60721", "ref_doc_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d"}, "3e0a49c7-46de-4ee8-88ba-67970956392a": {"doc_hash": "634cc1bf434f407acdbeccb36c488424c9e01307044e0c623d161f57c8434b8a", "ref_doc_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a"}, "d37f6a34-e101-4b16-94de-4a38310df900": {"doc_hash": "9a6d1edecd79e372af1cfdaafbac4e742140a2b37f53710c5c543e8242c94759", "ref_doc_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a"}, "fa5429df-0d36-47bf-98d0-6f05e95a1f9d": {"doc_hash": "3d159c190f16e4bcf82024c0e59186fc5faea4e8b41d073b9d7052d67a1a8f6c", "ref_doc_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a"}, "23a115f8-d036-44c4-be7c-693498c9edbc": {"doc_hash": "1f0d34029971d02da744ee32bbcc2057f1ffdab236a29ec8e566c2a9d0605a5a", "ref_doc_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816"}, "b72edb71-baa5-4ba2-ae5b-c95373a850e6": {"doc_hash": "432746639319cb5e5a67b576a52ac1a9fcb82cd68f00ee3d931093e19a30a36c", "ref_doc_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816"}, "1432a536-eeec-4566-98d2-1cbd204751f9": {"doc_hash": "9deb4e73444f1d419c404dcd6566665157c76662f91d9f649a9fce7e57c4ede9", "ref_doc_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816"}, "bdb6fc25-ee31-4991-9735-6dd34feafafe": {"doc_hash": "d2049abcdbc17679d09dde7aafd90844920379238ac66591f06e20dc0836b9e4", "ref_doc_id": "e06d64a7-ee72-41ba-b540-37b6012b609a"}, "7c56a9ae-811d-48a1-be78-9a55f9589edc": {"doc_hash": "4984cff93cbd9ebe431b86124bdfd7b0f9f431b51463679db100762d0c53bfb8", "ref_doc_id": "e06d64a7-ee72-41ba-b540-37b6012b609a"}, "39a03fb9-29f2-4816-a8de-2def39a9cb8d": {"doc_hash": "d2e1a756331ff310dea5597139f01c4f69e1c01025aea7dd8878235949c7bf6f", "ref_doc_id": "e06d64a7-ee72-41ba-b540-37b6012b609a"}, "9a3cbb06-4e2b-4865-afb3-d8e440e19c46": {"doc_hash": "4bc317c4a3228d4fb8b24fab3eb7a70649e1263bced9f6fb59671fa69bf71d34", "ref_doc_id": "260f8138-a208-4ccc-9a81-d2356237e0f4"}, "46cff651-ea21-41b1-9660-2365d3ca009e": {"doc_hash": "fb8e847d504735aa87698db21ab541c5017f97fbf847d5af88b0661a9e595347", "ref_doc_id": "260f8138-a208-4ccc-9a81-d2356237e0f4"}, "055062e8-b6a3-4422-af04-62fbbbdeecba": {"doc_hash": "4f5f54d3e4efabc89df3e4a5e9e2762003f3920463c1e6ab79baab617bef984c", "ref_doc_id": "260f8138-a208-4ccc-9a81-d2356237e0f4"}, "d8866c9b-042f-4a99-9ac5-966477cb54a5": {"doc_hash": "3fd4bf41182e6dccc161c878ce760ebddc3da10605c9493e86c7bd987924f671", "ref_doc_id": "7e642a29-edd4-4d99-ad44-20fe8d791868"}, "9d9431f6-c80c-41fc-90fd-66a0cf2008a1": {"doc_hash": "4ab560cbcbada6403324ecf34a90cb3c4f71d0cfa06822681f3fe60d1b3c346f", "ref_doc_id": "7e642a29-edd4-4d99-ad44-20fe8d791868"}, "10b3f15b-7311-4e9a-8949-393cbe0f92ee": {"doc_hash": "7f2fba83c0806f41c8e55fd089970d0e2fe01f42af29090670f5ca09a747da89", "ref_doc_id": "7e642a29-edd4-4d99-ad44-20fe8d791868"}}, "docstore/data": {"321e2c6e-20c1-40e8-a35e-b7cdd536cd8a": {"__data__": {"id_": "321e2c6e-20c1-40e8-a35e-b7cdd536cd8a", "embedding": null, "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a45e8efe58616a905fc92a8431e4e755e3cd258893e0c2e0a3d86aa311763fe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e13e9a-a506-405d-944a-40d86e70c828", "node_type": "1", "metadata": {}, "hash": "9c9dae3367571aef29efa5424623b9366424b0ca0e96da9a0da21333f014cfae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:1906.02243v1  [cs.CL]  5 Jun 2019\nEnergy and Policy Considerations for Deep Learning in NLP\nEmma Strubell Ananya Ganesh Andrew McCallum\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{strubell, aganesh, mccallum}@cs.umass.edu\nAbstract\nRecent progress in hardware and methodol-\nogy for training neural networks has ushered\nin a new generation of large networks trained\non abundant data. These models have ob-\ntained notable gains in accuracy across many\nNLP tasks. However, these accuracy improve-\nments depend on the availability of exception-\nally large computational resources that neces-\nsitate similarly substantial energy consump-\ntion. As a result these models are costly to\ntrain and develop, both \ufb01nancially, due to the\ncost of hardware and electricity or cloud com-\npute time, and environmentally, due to the car-\nbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring\nthis issue to the attention of NLP researchers\nby quantifying the approximate \ufb01nancial and\nenvironmental costs of training a variety of re-\ncently successful neural network models for\nNLP . Based on these \ufb01ndings, we propose ac-\ntionable recommendations to reduce costs and\nimprove equity in NLP research and practice.\n1 Introduction\nAdvances in techniques and hardware for train-\ning deep neural networks have recently en-\nabled impressive accuracy improvements across\nmany fundamental NLP tasks (\nBahdanau et al. ,\n2015; Luong et al. , 2015; Dozat and Man-\nning, 2017; V aswani et al. , 2017), with the\nmost computationally-hungry models obtaining\nthe highest scores (\nPeters et al. , 2018; Devlin et al. ,\n2019; Radford et al. , 2019; So et al. , 2019). As\na result, training a state-of-the-art model now re-\nquires substantial computational resources which\ndemand considerable energy , along with the as-\nsociated \ufb01nancial and environmental costs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1903, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62e13e9a-a506-405d-944a-40d86e70c828": {"__data__": {"id_": "62e13e9a-a506-405d-944a-40d86e70c828", "embedding": null, "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a45e8efe58616a905fc92a8431e4e755e3cd258893e0c2e0a3d86aa311763fe5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "321e2c6e-20c1-40e8-a35e-b7cdd536cd8a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "284b01c2a6a86820798d5127b4a4f13af35f753afcadc3e93c6249d6db0b48a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e9dcfbc-166d-402b-ad8a-722fa9395e4f", "node_type": "1", "metadata": {}, "hash": "ac428f0379c37f598c14e458dccdb2ee182beac56c18b4d92eca0aee9125c892", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", 2019; So et al. , 2019). As\na result, training a state-of-the-art model now re-\nquires substantial computational resources which\ndemand considerable energy , along with the as-\nsociated \ufb01nancial and environmental costs. Re-\nsearch and development of new models multiplies\nthese costs by thousands of times by requiring re-\ntraining to experiment with model architectures\nand hyperparameters. Whereas a decade ago most\nConsumption CO 2e (lbs)\nAir travel, 1 passenger, NY \u2194 SF 1984\nHuman life, avg, 1 year 11,023\nAmerican life, avg, 1 year 36,156\nCar, avg incl. fuel, 1 lifetime 126,000\nT raining one model (GPU)\nNLP pipeline (parsing, SRL) 39\nw/ tuning & experimentation 78,468\nTransformer (big) 192\nw/ neural architecture search 626,155\nT able 1: Estimated CO 2 emissions from training com-\nmon NLP models, compared to familiar consumption. 1\nNLP models could be trained and developed on\na commodity laptop or server, many now require\nmultiple instances of specialized hardware such as\nGPUs or TPUs, therefore limiting access to these\nhighly accurate models on the basis of \ufb01nances.\nEven when these expensive computational re-\nsources are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\ntions, and (2) when renewable energy is available,\nit is still limited to the equipment we have to pro-\nduce and store it, and energy spent training a neu-\nral network might better be allocated to heating a\nfamily\u2019s home.", "mimetype": "text/plain", "start_char_idx": 1682, "end_char_idx": 3434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e9dcfbc-166d-402b-ad8a-722fa9395e4f": {"__data__": {"id_": "4e9dcfbc-166d-402b-ad8a-722fa9395e4f", "embedding": null, "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d6a3726-fe4f-44db-8599-37bc8f5c261d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "a45e8efe58616a905fc92a8431e4e755e3cd258893e0c2e0a3d86aa311763fe5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62e13e9a-a506-405d-944a-40d86e70c828", "node_type": "1", "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "03ac5d2f0e4c769005c734ab37f75781b56d703957dd29750e829e78fb1db779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is estimated that we must cut\ncarbon emissions by half over the next decade to\ndeter escalating rates of natural disaster, and based\non the estimated CO2 emissions listed in T able\n1,\n1 Sources: (1) Air travel and per-capita consump-\ntion: https://bit.ly/2Hw0xWc; (2) car lifetime:\nhttps://bit.ly/2Qbr0w1.", "mimetype": "text/plain", "start_char_idx": 3435, "end_char_idx": 3743, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e0a49c7-46de-4ee8-88ba-67970956392a": {"__data__": {"id_": "3e0a49c7-46de-4ee8-88ba-67970956392a", "embedding": null, "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "aa5fdd9b0b04279466fb47e2314c4ab7b48df0a9b4be99010c70ea956a621c8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d37f6a34-e101-4b16-94de-4a38310df900", "node_type": "1", "metadata": {}, "hash": "fd3c0844d4b26c7214eabf176becb0ab7373a1cd07e91276231ddefdf1495293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nT o heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy , we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. W e do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. T o estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline (\nStrubell et al. , 2018). W e conclude\nwith recommendations to the community based on\nour \ufb01ndings, namely: (1) Time to retrain and sen-\nsitivity to hyperparameters should be reported for\nNLP machine learning models; (2) academic re-\nsearchers need equitable access to computational\nresources; and (3) researchers should prioritize de-\nveloping ef\ufb01cient models and hardware.\n2 Methods\nT o quantify the computational and environmen-\ntal cost of training deep neural network mod-\nels for NLP , we perform an analysis of the en-\nergy required to train a variety of popular off-\nthe-shelf NLP models, as well as a case study of\nthe complete sum of resources required to develop\nLISA (\nStrubell et al. , 2018), a state-of-the-art NLP\nmodel from EMNLP 2018, including all tuning\nand experimentation.\nW e measure energy use as follows. W e train the\nmodels described in \u00a7\n2.1 using the default settings\nprovided, and sample GPU and CPU power con-\nsumption during training. Each model was trained\nfor a maximum of 1 day . W e train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d37f6a34-e101-4b16-94de-4a38310df900": {"__data__": {"id_": "d37f6a34-e101-4b16-94de-4a38310df900", "embedding": null, "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "aa5fdd9b0b04279466fb47e2314c4ab7b48df0a9b4be99010c70ea956a621c8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e0a49c7-46de-4ee8-88ba-67970956392a", "node_type": "1", "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "634cc1bf434f407acdbeccb36c488424c9e01307044e0c623d161f57c8434b8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa5429df-0d36-47bf-98d0-6f05e95a1f9d", "node_type": "1", "metadata": {}, "hash": "22b2ed9aea056218d8a10a7c7edea6fe1322508fb9d6b0deb9efc4267adb73a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each model was trained\nfor a maximum of 1 day . W e train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface\n2 to sample the GPU power consumption\nand report the average over all samples. T o sample\nCPU power consumption, we use Intel\u2019s Running\nA verage Power Limit interface.\n3\n2 nvidia-smi: https://bit.ly/30sGEbi\n3 RAPL power meter: https://bit.ly/2LObQhV\nConsumer Renew . Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-A WS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\nT able 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (\nCook et al. , 2017),\ncompared to the United States, 4 China5 and Germany\n(Burger, 2019).\nW e estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. W e then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Letpc be the average power\ndraw (in watts) from all CPU sockets during train-\ning, letpr be the average power draw from all\nDRAM (main memory) sockets, let pg be the aver-\nage power draw of a GPU during training, and let\ngbe the number of GPUs used to train. W e esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).", "mimetype": "text/plain", "start_char_idx": 1813, "end_char_idx": 3458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa5429df-0d36-47bf-98d0-6f05e95a1f9d": {"__data__": {"id_": "fa5429df-0d36-47bf-98d0-6f05e95a1f9d", "embedding": null, "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9e30e57-a68e-4e42-ad64-192b8bd29d8a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "aa5fdd9b0b04279466fb47e2314c4ab7b48df0a9b4be99010c70ea956a621c8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d37f6a34-e101-4b16-94de-4a38310df900", "node_type": "1", "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "9a6d1edecd79e372af1cfdaafbac4e742140a2b37f53710c5c543e8242c94759", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "W e esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).\nW e use a PUE coef\ufb01cient of 1.58, the 2018 global\naverage for data centers (\nAscierto, 2018). Then the\ntotal power pt required at a given instance during\ntraining is given by:\npt = 1.58t(pc + pr + gpg)\n1000 (1)\nThe U.S. Environmental Protection Agency (EP A)\nprovides average CO2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.\n(\nEP A, 2018), which we use to convert power to\nestimated CO 2 emissions:\nCO2e = 0.954pt (2)\nThis conversion takes into account the relative pro-\nportions of different energy sources (primarily nat-\nural gas, coal, nuclear and renewable) consumed\nto produce energy in the United States. T able\n2\nlists the relative energy sources for China, Ger-\nmany and the United States compared to the top\n5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI\n5 China Electricity Council; trans. China Energy Portal:\nhttps://bit.ly/2QHE5O3", "mimetype": "text/plain", "start_char_idx": 3212, "end_char_idx": 4328, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23a115f8-d036-44c4-be7c-693498c9edbc": {"__data__": {"id_": "23a115f8-d036-44c4-be7c-693498c9edbc", "embedding": null, "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816", "node_type": "4", "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5a7bd5b0689e1af7970058e64ad7df6ea493f1d5c6480f6c89cb300a330697d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72edb71-baa5-4ba2-ae5b-c95373a850e6", "node_type": "1", "metadata": {}, "hash": "a93330f0f261f67ebd86e4c26bc6ad48cd7048b8bb52ea87fcb6e1d52c0b14e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "three cloud service providers. The U.S. break-\ndown of energy is comparable to that of the most\npopular cloud compute service, Amazon W eb Ser-\nvices, so we believe this conversion to provide a\nreasonable estimate of CO2 emissions per kilowatt\nhour of compute energy used.\n2.1 Models\nW e analyze four models, the computational re-\nquirements of which we describe below . All mod-\nels have code freely available online, which we\nused out-of-the-box. For more details on the mod-\nels themselves, please refer to the original papers.\nT ransformer. The Transformer model (\nV aswani\net al. , 2017) is an encoder-decoder architecture\nprimarily recognized for ef\ufb01cient and accurate ma-\nchine translation. The encoder and decoder each\nconsist of 6 stacked layers of multi-head self-\nattention.\nV aswani et al. (2017) report that the\nTransformer base model (65M parameters) was\ntrained on 8 NVIDIA P100 GPUs for 12 hours,\nand the Transformerbig model (213M parame-\nters) was trained for 3.5 days (84 hours; 300k\nsteps). This model is also the basis for recent\nwork on neural architecture search (NAS) for ma-\nchine translation and language modeling (\nSo et al. ,\n2019), and the NLP pipeline that we study in more\ndetail in \u00a74.2 (Strubell et al. , 2018). So et al.\n(2019) report that their full architecture search ran\nfor a total of 979M training steps, and that their\nbase model requires 10 hours to train for 300k\nsteps on one TPUv2 core. This equates to 32,623\nhours of TPU or 274,120 hours on 8 P100 GPUs.\nELMo.The ELMo model (\nPeters et al. , 2018)\nis based on stacked LSTMs and provides rich\nword representations in context by pre-training on\na large amount of data using a language model-\ning objective.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b72edb71-baa5-4ba2-ae5b-c95373a850e6": {"__data__": {"id_": "b72edb71-baa5-4ba2-ae5b-c95373a850e6", "embedding": null, "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816", "node_type": "4", "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5a7bd5b0689e1af7970058e64ad7df6ea493f1d5c6480f6c89cb300a330697d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23a115f8-d036-44c4-be7c-693498c9edbc", "node_type": "1", "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "1f0d34029971d02da744ee32bbcc2057f1ffdab236a29ec8e566c2a9d0605a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1432a536-eeec-4566-98d2-1cbd204751f9", "node_type": "1", "metadata": {}, "hash": "0d727062588b60f23ffedb812df528ebd2c25c33f93448ead2e70349cfdc11bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ELMo.The ELMo model (\nPeters et al. , 2018)\nis based on stacked LSTMs and provides rich\nword representations in context by pre-training on\na large amount of data using a language model-\ning objective. Replacing context-independent pre-\ntrained word embeddings with ELMo has been\nshown to increase performance on downstream\ntasks such as named entity recognition, semantic\nrole labeling, and coreference.\nPeters et al. (2018)\nreport that ELMo was trained on 3 NVIDIA GTX\n1080 GPUs for 2 weeks (336 hours).\nBERT .The BERT model (\nDevlin et al. , 2019) pro-\nvides a Transformer-based architecture for build-\ning contextual representations similar to ELMo,\nbut trained with a different language modeling ob-\njective. BERT substantially improves accuracy on\ntasks requiring sentence-level representations such\nas question answering and natural language infer-\nence.\nDevlin et al. (2019) report that the BERT\nbase model (110M parameters) was trained on 16\nTPU chips for 4 days (96 hours). NVIDIA reports\nthat they can train a BERT model in 3.3 days (79.2\nhours) using 4 DGX-2H servers, totaling 64 T esla\nV100 GPUs (\nForster et al. , 2019).\nGPT -2. This model is the latest edition of\nOpenAI\u2019s GPT general-purpose token encoder,\nalso based on Transformer-style self-attention and\ntrained with a language modeling objective (\nRad-\nford et al. , 2019). By training a very large model\non massive data, Radford et al. (2019) show high\nzero-shot performance on question answering and\nlanguage modeling benchmarks. The large model\ndescribed in\nRadford et al. (2019) has 1542M pa-\nrameters and is reported to require 1 week (168\nhours) of training on 32 TPUv3 chips.\n6\n3 Related work\nThere is some precedent for work characterizing\nthe computational requirements of training and in-\nference in modern neural network architectures in\nthe computer vision community .\nLi et al.", "mimetype": "text/plain", "start_char_idx": 1501, "end_char_idx": 3362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1432a536-eeec-4566-98d2-1cbd204751f9": {"__data__": {"id_": "1432a536-eeec-4566-98d2-1cbd204751f9", "embedding": null, "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a03b08c2-81f0-49e1-b2f0-30c5f54ae816", "node_type": "4", "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "5a7bd5b0689e1af7970058e64ad7df6ea493f1d5c6480f6c89cb300a330697d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72edb71-baa5-4ba2-ae5b-c95373a850e6", "node_type": "1", "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "432746639319cb5e5a67b576a52ac1a9fcb82cd68f00ee3d931093e19a30a36c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\n3 Related work\nThere is some precedent for work characterizing\nthe computational requirements of training and in-\nference in modern neural network architectures in\nthe computer vision community .\nLi et al. (2016)\npresent a detailed study of the energy use required\nfor training and inference in popular convolutional\nmodels for image classi\ufb01cation in computer vi-\nsion, including \ufb01ne-grained analysis comparing\ndifferent neural network layer types.\nCanziani\net al. (2016) assess image classi\ufb01cation model ac-\ncuracy as a function of model size and giga\ufb02ops\nrequired during inference. They also measure av-\nerage power draw required during inference on\nGPUs as a function of batch size. Neither work an-\nalyzes the recurrent and self-attention models that\nhave become commonplace in NLP , nor do they\nextrapolate power to estimates of carbon and dol-\nlar cost of training.\nAnalysis of hyperparameter tuning has been\nperformed in the context of improved algorithms\nfor hyperparameter search (\nBergstra et al. , 2011;\nBergstra and Bengio , 2012; Snoek et al. , 2012). T o\nour knowledge there exists to date no analysis of\nthe computation required for R&D and hyperpa-\nrameter tuning of neural network models in NLP .\n6 Via the authors on Reddit .\n7 GPU lower bound computed using pre-emptible\nP100/V100 U.S. resources priced at $0.43\u2013$0.74/hr, upper\nbound uses on-demand U.S. resources priced at $1.46\u2013\n$2.48/hr. W e similarly use pre-emptible ($1.46/hr\u2013$2.40/hr)\nand on-demand ($4.50/hr\u2013$8/hr) pricing as lower and upper\nbounds for TPU v2/3; cheaper bulk contracts are available.", "mimetype": "text/plain", "start_char_idx": 3155, "end_char_idx": 4734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdb6fc25-ee31-4991-9735-6dd34feafafe": {"__data__": {"id_": "bdb6fc25-ee31-4991-9735-6dd34feafafe", "embedding": null, "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e06d64a7-ee72-41ba-b540-37b6012b609a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "e36d4f70e598805b688afc95d9170d8318d0a28d9bb1bc0dacfb0f5d814f8795", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c56a9ae-811d-48a1-be78-9a55f9589edc", "node_type": "1", "metadata": {}, "hash": "a93076ac13f9935bbd8e02d8fef8184306fb5e2065bbc65643f8823252a2493b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Hardware Power (W) Hours kWh \u00b7PUE CO 2e Cloud compute cost\nTransformerbase P100x8 1415.78 12 27 26 $41\u2013$140\nTransformerbig P100x8 1515.43 84 201 192 $289\u2013$981\nELMo P100x3 517.66 336 275 262 $433\u2013$1472\nBERTbase V100x64 12,041.51 79 1507 1438 $3751\u2013$12,571\nBERTbase TPUv2x16 \u2014 96 \u2014 \u2014 $2074\u2013$6912\nNAS P100x8 1515.43 274,120 656,347 626,155 $942,973\u2013$3,20 1,722\nNAS TPUv2x1 \u2014 32,623 \u2014 \u2014 $44,055\u2013$146,848\nGPT -2 TPUv3x32 \u2014 168 \u2014 \u2014 $12,902\u2013$43,008\nT able 3: Estimated cost of training a model in terms of CO 2 emissions (lbs) and cloud compute cost (USD).\n7 Power\nand carbon footprint are omitted for TPUs due to lack of publi c information on power draw for this hardware.\n4 Experimental results\n4.1 Cost of training\nT able\n3 lists CO 2 emissions and estimated cost of\ntraining the models described in \u00a72.1. Of note is\nthat TPUs are more cost-ef\ufb01cient than GPUs on\nworkloads that make sense for that hardware (e.g.\nBERT). W e also see that models emit substan-\ntial carbon emissions; training BERT on GPU is\nroughly equivalent to a trans-American \ufb02ight.\nSo\net al. (2019) report that NAS achieves a new state-\nof-the-art BLEU score of 29.7 for English to Ger-\nman machine translation, an increase of just 0.1\nBLEU at the cost of at least $150k in on-demand\ncompute time and non-trivial carbon emissions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c56a9ae-811d-48a1-be78-9a55f9589edc": {"__data__": {"id_": "7c56a9ae-811d-48a1-be78-9a55f9589edc", "embedding": null, "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e06d64a7-ee72-41ba-b540-37b6012b609a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "e36d4f70e598805b688afc95d9170d8318d0a28d9bb1bc0dacfb0f5d814f8795", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdb6fc25-ee31-4991-9735-6dd34feafafe", "node_type": "1", "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "d2049abcdbc17679d09dde7aafd90844920379238ac66591f06e20dc0836b9e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39a03fb9-29f2-4816-a8de-2def39a9cb8d", "node_type": "1", "metadata": {}, "hash": "cc4548d5a020ce6f4f46cac47e27209a4701f421ed149ab32728e1f69199eba5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2 Cost of development: Case study\nT o quantify the computational requirements of\nR&D for a new model we study the logs of\nall training required to develop Linguistically-\nInformed Self-Attention (\nStrubell et al. , 2018), a\nmulti-task model that performs part-of-speech tag-\nging, labeled dependency parsing, predicate detec-\ntion and semantic role labeling. This model makes\nfor an interesting case study as a representative\nNLP pipeline and as a Best Long Paper at EMNLP .\nModel training associated with the project\nspanned a period of 172 days (approx. 6 months).\nDuring that time 123 small hyperparameter grid\nsearches were performed, resulting in 4789 jobs\nin total. Jobs varied in length ranging from a min-\nimum of 3 minutes, indicating a crash, to a maxi-\nmum of 9 days, with an average job length of 52\nhours. All training was done on a combination of\nNVIDIA Titan X (72%) and M40 (28%) GPUs.\n8\nThe sum GPU time required for the project\ntotaled 9998 days (27 years). This averages to\n8 W e approximate cloud compute cost using P100 pricing.\nEstimated cost (USD)\nModels Hours Cloud compute Electricity\n1 120 $52\u2013$175 $5\n24 2880 $1238\u2013$4205 $118\n4789 239,942 $103k\u2013$350k $9870\nT able 4: Estimated cost in terms of cloud compute and\nelectricity for training: (1) a single model (2) a single\ntune and (3) all models trained during R&D.\nabout 60 GPUs running constantly throughout the\n6 month duration of the project. T able\n4 lists upper\nand lower bounds of the estimated cost in terms\nof Google Cloud compute and raw electricity re-\nquired to develop and deploy this model.\n9 W e see\nthat while training a single model is relatively in-\nexpensive, the cost of tuning a model for a new\ndataset, which we estimate here to require 24 jobs,\nor performing the full R&D required to develop\nthis model, quickly becomes extremely expensive.\n5 Conclusions\nAuthors should report training time and\nsensitivity to hyperparameters.", "mimetype": "text/plain", "start_char_idx": 1304, "end_char_idx": 3230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39a03fb9-29f2-4816-a8de-2def39a9cb8d": {"__data__": {"id_": "39a03fb9-29f2-4816-a8de-2def39a9cb8d", "embedding": null, "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e06d64a7-ee72-41ba-b540-37b6012b609a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "e36d4f70e598805b688afc95d9170d8318d0a28d9bb1bc0dacfb0f5d814f8795", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c56a9ae-811d-48a1-be78-9a55f9589edc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "4984cff93cbd9ebe431b86124bdfd7b0f9f431b51463679db100762d0c53bfb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 Conclusions\nAuthors should report training time and\nsensitivity to hyperparameters.\nOur experiments suggest that it would be bene\ufb01-\ncial to directly compare different models to per-\nform a cost-bene\ufb01t (accuracy) analysis. T o ad-\ndress this, when proposing a model that is meant\nto be re-trained for downstream use, such as re-\ntraining on a new domain or \ufb01ne-tuning on a new\ntask, authors should report training time and com-\nputational resources required, as well as model\nsensitivity to hyperparameters. This will enable\ndirect comparison across models, allowing subse-\nquent consumers of these models to accurately as-\nsess whether the required computational resources\n9 Based on average U.S cost of electricity of $0.12/kWh.", "mimetype": "text/plain", "start_char_idx": 3145, "end_char_idx": 3876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a3cbb06-4e2b-4865-afb3-d8e440e19c46": {"__data__": {"id_": "9a3cbb06-4e2b-4865-afb3-d8e440e19c46", "embedding": null, "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "260f8138-a208-4ccc-9a81-d2356237e0f4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "dc6ff39107002a9b3b9a71ec4826ebff0a278c0e47da38782068e78d43932167", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46cff651-ea21-41b1-9660-2365d3ca009e", "node_type": "1", "metadata": {}, "hash": "3c47dd319e3e69db7a2e53a1f0dcc1b4c9e03c709cf24ed7fc9c947efb37251a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "are compatible with their setting. More explicit\ncharacterization of tuning time could also reveal\ninconsistencies in time spent tuning baseline mod-\nels compared to proposed contributions. Realiz-\ning this will require: (1) a standard, hardware-\nindependent measurement of training time, such\nas giga\ufb02ops required to convergence, and (2) a\nstandard measurement of model sensitivity to data\nand hyperparameters, such as variance with re-\nspect to hyperparameters searched.\nAcademic researchers need equitable access to\ncomputation resources.\nRecent advances in available compute come at a\nhigh price not attainable to all who desire access.\nMost of the models studied in this paper were de-\nveloped outside academia; recent improvements in\nstate-of-the-art accuracy are possible thanks to in-\ndustry access to large-scale compute.\nLimiting this style of research to industry labs\nhurts the NLP research community in many ways.\nFirst, it sti\ufb02es creativity . Researchers with good\nideas but without access to large-scale compute\nwill simply not be able to execute their ideas,\ninstead constrained to focus on different prob-\nlems. Second, it prohibits certain types of re-\nsearch on the basis of access to \ufb01nancial resources.\nThis even more deeply promotes the already prob-\nlematic \u201crich get richer\u201d cycle of research fund-\ning, where groups that are already successful and\nthus well-funded tend to receive more funding\ndue to their existing accomplishments. Third, the\nprohibitive start-up cost of building in-house re-\nsources forces resource-poor groups to rely on\ncloud compute services such as A WS, Google\nCloud and Microsoft Azure.\nWhile these services provide valuable, \ufb02exi-\nble, and often relatively environmentally friendly\ncompute resources, it is more cost effective for\nacademic researchers, who often work for non-\npro\ufb01t educational institutions and whose research\nis funded by government entities, to pool resources\nto build shared compute centers at the level of\nfunding agencies, such as the U.S. National Sci-\nence Foundation. For example, an off-the-shelf\nGPU server containing 8 NVIDIA 1080 Ti GPUs\nand supporting hardware can be purchased for\napproximately $20,000 USD.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46cff651-ea21-41b1-9660-2365d3ca009e": {"__data__": {"id_": "46cff651-ea21-41b1-9660-2365d3ca009e", "embedding": null, "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "260f8138-a208-4ccc-9a81-d2356237e0f4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "dc6ff39107002a9b3b9a71ec4826ebff0a278c0e47da38782068e78d43932167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a3cbb06-4e2b-4865-afb3-d8e440e19c46", "node_type": "1", "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "4bc317c4a3228d4fb8b24fab3eb7a70649e1263bced9f6fb59671fa69bf71d34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055062e8-b6a3-4422-af04-62fbbbdeecba", "node_type": "1", "metadata": {}, "hash": "c4899b323e17b2bf165863e16649621226b7d05ca6011057107cf204ab369edb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "National Sci-\nence Foundation. For example, an off-the-shelf\nGPU server containing 8 NVIDIA 1080 Ti GPUs\nand supporting hardware can be purchased for\napproximately $20,000 USD. At that cost, the\nhardware required to develop the model in our\ncase study (approximately 58 GPUs for 172 days)\nwould cost $145,000 USD plus electricity , about\nhalf the estimated cost to use on-demand cloud\nGPUs. Unlike money spent on cloud compute,\nhowever, that invested in centralized resources\nwould continue to pay off as resources are shared\nacross many projects. A government-funded aca-\ndemic compute cloud would provide equitable ac-\ncess to all researchers.\nResearchers should prioritize computationally\nef\ufb01cient hardware and algorithms.\nW e recommend a concerted effort by industry and\nacademia to promote research of more computa-\ntionally ef\ufb01cient algorithms, as well as hardware\nthat requires less energy . An effort can also be\nmade in terms of software. There is already a\nprecedent for NLP software packages prioritizing\nef\ufb01cient models. An additional avenue through\nwhich NLP and machine learning software de-\nvelopers could aid in reducing the energy asso-\nciated with model tuning is by providing easy-\nto-use APIs implementing more ef\ufb01cient alterna-\ntives to brute-force grid search for hyperparameter\ntuning, e.g. random or Bayesian hyperparameter\nsearch techniques (\nBergstra et al. , 2011; Bergstra\nand Bengio , 2012; Snoek et al. , 2012). While\nsoftware packages implementing these techniques\ndo exist,\n10 they are rarely employed in practice\nfor tuning NLP models. This is likely because\ntheir interoperability with popular deep learning\nframeworks such as PyT orch and T ensorFlow is\nnot optimized, i.e. there are not simple exam-\nples of how to tune T ensorFlow Estimators using\nBayesian search. Integrating these tools into the\nwork\ufb02ows with which NLP researchers and practi-\ntioners are already familiar could have notable im-\npact on the cost of developing and tuning in NLP .", "mimetype": "text/plain", "start_char_idx": 2014, "end_char_idx": 3999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "055062e8-b6a3-4422-af04-62fbbbdeecba": {"__data__": {"id_": "055062e8-b6a3-4422-af04-62fbbbdeecba", "embedding": null, "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "260f8138-a208-4ccc-9a81-d2356237e0f4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "dc6ff39107002a9b3b9a71ec4826ebff0a278c0e47da38782068e78d43932167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46cff651-ea21-41b1-9660-2365d3ca009e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "fb8e847d504735aa87698db21ab541c5017f97fbf847d5af88b0661a9e595347", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Integrating these tools into the\nwork\ufb02ows with which NLP researchers and practi-\ntioners are already familiar could have notable im-\npact on the cost of developing and tuning in NLP .\nAcknowledgements\nW e are grateful to Sherief Farouk and the anony-\nmous reviewers for helpful feedback on earlier\ndrafts. This work was supported in part by the\nCenters for Data Science and Intelligent Infor-\nmation Retrieval, the Chan Zuckerberg Initiative\nunder the Scienti\ufb01c Knowledge Base Construc-\ntion project, the IBM Cognitive Horizons Network\nagreement no. W1668553, and National Science\nFoundation grant no. IIS-1514053. Any opinions,\n\ufb01ndings and conclusions or recommendations ex-\npressed in this material are those of the authors and\ndo not necessarily re\ufb02ect those of the sponsor.\n10 For example, the Hyperopt Python library .", "mimetype": "text/plain", "start_char_idx": 3816, "end_char_idx": 4639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8866c9b-042f-4a99-9ac5-966477cb54a5": {"__data__": {"id_": "d8866c9b-042f-4a99-9ac5-966477cb54a5", "embedding": null, "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e642a29-edd4-4d99-ad44-20fe8d791868", "node_type": "4", "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "777aa79895df50ac8b4b2d3073db88963a35209a1f29460ba562df36e96074b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d9431f6-c80c-41fc-90fd-66a0cf2008a1", "node_type": "1", "metadata": {}, "hash": "d6a872aec78cc4f3034430fb1d4ae28d866583073a3139f5ef0845d688ec43d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\nRhonda Ascierto. 2018.\nUptime Institute Global Data\nCenter Survey . T echnical report, Uptime Institute.\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In3rd Inter-\nnational Conference for Learning Representations\n(ICLR), San Diego, California, USA.\nJames Bergstra and Y oshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. Journal of\nMachine Learning Research, 13(Feb):281\u2013305.\nJames S Bergstra, R\u00b4 emi Bardenet, Y oshua Bengio, and\nBal\u00b4 azs K \u00b4 egl. 2011. Algorithms for hyper-parameter\noptimization. InAdvances in neural information\nprocessing systems, pages 2546\u20132554.\nBruno Burger. 2019.\nNet Public Electricity Generation\nin Germany in 2018 . T echnical report, Fraunhofer\nInstitute for Solar Energy Systems ISE.\nAlfredo Canziani, Adam Paszke, and Eugenio Culur-\nciello. 2016. An analysis of deep neural network\nmodels for practical applications .\nGary Cook, Jude Lee, T amina Tsai, Ada Kongn, John\nDeans, Brian Johnson, Elizabeth Jardim, and Brian\nJohnson. 2017.\nClicking Clean: Who is winning\nthe race to build a green internet? T echnical report,\nGreenpeace.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. InNAACL.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaf\ufb01ne attention for neural dependency pars-\ning. InICLR.\nEP A. 2018.\nEmissions & Generation Resource Inte-\ngrated Database (eGRID) . T echnical report, U.S.\nEnvironmental Protection Agency.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d9431f6-c80c-41fc-90fd-66a0cf2008a1": {"__data__": {"id_": "9d9431f6-c80c-41fc-90fd-66a0cf2008a1", "embedding": null, "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e642a29-edd4-4d99-ad44-20fe8d791868", "node_type": "4", "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "777aa79895df50ac8b4b2d3073db88963a35209a1f29460ba562df36e96074b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8866c9b-042f-4a99-9ac5-966477cb54a5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "3fd4bf41182e6dccc161c878ce760ebddc3da10605c9493e86c7bd987924f671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b3f15b-7311-4e9a-8949-393cbe0f92ee", "node_type": "1", "metadata": {}, "hash": "354b7b051d3c2f53eee3584f98d48bf6af3fcf331edf80c413adb21e73d8b9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2017.\nDeep biaf\ufb01ne attention for neural dependency pars-\ning. InICLR.\nEP A. 2018.\nEmissions & Generation Resource Inte-\ngrated Database (eGRID) . T echnical report, U.S.\nEnvironmental Protection Agency.\nChristopher Forster, Thor Johnsen, Swetha Man-\ndava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie\nBernauer, Allison Gray, Sharan Chetlur, and Raul\nPuri. 2019.\nBER T Meets GPUs . T echnical report,\nNVIDIA AI.\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong.\n2016. Evaluating the energy ef\ufb01ciency of deep con-\nvolutional neural networks on cpus and gpus.2016\nIEEE International Conferences on Big Data and\nCloud Computing (BDCloud), Social Computing\nand Networking (SocialCom), Sustainable Comput-\ning and Communications (SustainCom) (BDCloud-\nSocialCom-SustainCom), pages 477\u2013484.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015.\nEffective approaches to attention-based\nneural machine translation . In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412\u20131421. Associa-\ntion for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. InNAACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019.\nLanguage\nmodels are unsupervised multitask learners .\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. InAdvances in neural informa-\ntion processing systems, pages 2951\u20132959.\nDavid R. So, Chen Liang, and Quoc V . Le. 2019.\nThe evolved transformer .", "mimetype": "text/plain", "start_char_idx": 1388, "end_char_idx": 3038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10b3f15b-7311-4e9a-8949-393cbe0f92ee": {"__data__": {"id_": "10b3f15b-7311-4e9a-8949-393cbe0f92ee", "embedding": null, "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e642a29-edd4-4d99-ad44-20fe8d791868", "node_type": "4", "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "777aa79895df50ac8b4b2d3073db88963a35209a1f29460ba562df36e96074b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d9431f6-c80c-41fc-90fd-66a0cf2008a1", "node_type": "1", "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}, "hash": "4ab560cbcbada6403324ecf34a90cb3c4f71d0cfa06822681f3fe60d1b3c346f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2012. Practical bayesian optimization of machine\nlearning algorithms. InAdvances in neural informa-\ntion processing systems, pages 2951\u20132959.\nDavid R. So, Chen Liang, and Quoc V . Le. 2019.\nThe evolved transformer . In Proceedings of the\n36th International Conference on Machine Learning\n(ICML).\nEmma Strubell, Patrick V erga, Daniel Andor,\nDavid W eiss, and Andrew McCallum. 2018.\nLinguistically-Informed Self-Attention for Se-\nmantic Role Labeling. InConference on Empir-\nical Methods in Natural Language Processing\n(EMNLP), Brussels, Belgium.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In31st Conference on Neural Information\nProcessing Systems (NIPS).", "mimetype": "text/plain", "start_char_idx": 2823, "end_char_idx": 3594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"7d6a3726-fe4f-44db-8599-37bc8f5c261d": {"node_ids": ["321e2c6e-20c1-40e8-a35e-b7cdd536cd8a", "62e13e9a-a506-405d-944a-40d86e70c828", "4e9dcfbc-166d-402b-ad8a-722fa9395e4f"], "metadata": {"page_label": "1", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "e9e30e57-a68e-4e42-ad64-192b8bd29d8a": {"node_ids": ["3e0a49c7-46de-4ee8-88ba-67970956392a", "d37f6a34-e101-4b16-94de-4a38310df900", "fa5429df-0d36-47bf-98d0-6f05e95a1f9d"], "metadata": {"page_label": "2", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "a03b08c2-81f0-49e1-b2f0-30c5f54ae816": {"node_ids": ["23a115f8-d036-44c4-be7c-693498c9edbc", "b72edb71-baa5-4ba2-ae5b-c95373a850e6", "1432a536-eeec-4566-98d2-1cbd204751f9"], "metadata": {"page_label": "3", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "e06d64a7-ee72-41ba-b540-37b6012b609a": {"node_ids": ["bdb6fc25-ee31-4991-9735-6dd34feafafe", "7c56a9ae-811d-48a1-be78-9a55f9589edc", "39a03fb9-29f2-4816-a8de-2def39a9cb8d"], "metadata": {"page_label": "4", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "260f8138-a208-4ccc-9a81-d2356237e0f4": {"node_ids": ["9a3cbb06-4e2b-4865-afb3-d8e440e19c46", "46cff651-ea21-41b1-9660-2365d3ca009e", "055062e8-b6a3-4422-af04-62fbbbdeecba"], "metadata": {"page_label": "5", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}, "7e642a29-edd4-4d99-ad44-20fe8d791868": {"node_ids": ["d8866c9b-042f-4a99-9ac5-966477cb54a5", "9d9431f6-c80c-41fc-90fd-66a0cf2008a1", "10b3f15b-7311-4e9a-8949-393cbe0f92ee"], "metadata": {"page_label": "6", "file_name": "J3.pdf", "file_path": "uploaded_pdfs/J3.pdf", "file_type": "application/pdf", "file_size": 113887, "creation_date": "2025-04-14", "last_modified_date": "2025-04-14"}}}}